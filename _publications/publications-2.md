---
layout: archive
title: "Code-switching as a cross-lingual Training Signal: an Example with Unsupervised Bilingual Embedding"
collection: publications
---

_Gaschi et al. 2023_  
Workshop paper - MRL@EMNLP2023

<style>
    form button {
        background-color: #4CAF50; /* Green background color */
        color: white; /* White text color */
        padding: 10px 15px; /* Padding inside the button */
        border: none; /* No border */
        border-radius: 5px; /* Rounded corners */
        cursor: pointer; /* Cursor style on hover */
    }

    /* Style for the second button */
    form:nth-child(2) button {
        background-color: #008CBA; /* Blue background color */
    }
</style>

<td>
    <nobr>
        <form style="float: left; width=150px; margin-right: 10px;" action="https://aclanthology.org/2023.mrl-1.16.pdf" method="get" target="_blank"><button type="submit">PDF</button></form> 
        <form style="float: none; width=150px" action="https://aclanthology.org/2023.mrl-1.16/" method="get" target="_blank"><button type="submit">Cite</button></form>
    </nobr>
</td>  

## Abstract

Code-switching is the occurrence of words from different languages in the same utterance. This paper shows that code-switching is largely present in a popular dataset for training word embeddings, and demonstrates that it can be a useful training signal for unsupervised cross-lingual embeddings. \ours, the proposed method for leveraging this signal, outperforms other unsupervised mapping-based methods for cross-lingual embeddings on two of the three tested language pairs and suggests that code-switching can be a useful training signal for multilingual representations.

*Keywords: code-switching, word embeddings, multilingual alignment, unsupervised mapping*
